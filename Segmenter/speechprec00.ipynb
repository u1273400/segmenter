{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Characteristics\n",
    "\n",
    "*Despite many differences between individuals, and existence of many languages, speech folows general patternas, and on average has well defined characteristics such as those of volume, frequency distribution, pitch rate and syllabic rate.*(McLoughlin, 2009) \n",
    "\n",
    "### Speech Classification\n",
    "*Physically, sounds of speech can be described in terms of a pitch countour and format frequencies.  In fact this description forms a method of analysis used by most speech compression algorithms.  Formants are resonant frequencies of the vocal tract which appear in the speech spectrum as clear peaks.  As an example, three distinct formant speaks can bee seen inthe frequency domain plot of a short speech recording shown in figure 1 below.*\n",
    "![Figure 1: Formants](https://selene.hud.ac.uk/u1273400/images/seg_media/formants.PNG)\n",
    "**Figure 1:** Spectrum plot of a 20ms recording of speech, showing three distinct peaks (McLoughlin, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Formants have been described by the famous research Klatt and others as single most important feature in speech communications.  Generally many formants will be present in a typical utterance, and the location of these will vary over time a the shape of the mouth changes.  Formants are counted from the lowest frequency upwards, and usually only the first three (F1, F2 and F3) contribute significantly to the intelligibility of speech.  Some fricative sounds like /ch/ can produce lots of formants but generally speaking F1 contains most of the speeche energy while F2 and F3 contribute more to speech intelligibility*.\n",
    "\n",
    "*The pitch contour (often called f0 - note the lower case notation) is the parameter that describes the tone of the voice (the perceived frequency) and it is in effect the fondamental vocal frequency.  Again, pitch frequencies contain energy but contribute little to intelligibility for English and European langauges.  It is however, a very different matter in a tonal language such as Mandarin Chinese which is totally dependent on tone for conveying meaning.  As an example, in Chinese the single word 'ma' can mean one of five things depending on which tone it is spoken with: mother, horse, scold, question, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude distribution of speech\n",
    "*The overall amplitude distribution of speech depends on the speaker's personality and mood (every reader is likely to have endured monotonous talks on occasion - literally meaning 'single tone' speech), environmental noise, infection and so on.*\n",
    "\n",
    "*Also, feedback from a listener, either verbal 'speak up please' or non-verbal such as cupping hand around an ear can prompt a speaker to alter their vocal characteristics.  However, despite this variability, itis interesting to determine average speech levels in different environments.*\n",
    "\n",
    "*In general, as the noise level increases by 1 dB, a speaker will raise his voice level by 0.5 dB within the range of normal speech.  With very low noise levels, a male adult speaker can produce 52 dB<sub>SPL</sub> of speech measured at a distance of 1m when speaking casually.  This raises to about 90 dB<sub>SPL</sub> when shouting.  The dynamic range of conversational speech is around 30 dB<sub>SPL</sub>, and the mean level for males measured at 1m is somewhere in the region of 55-60 dBA<sub>SPL</sub> ('A' refers to the perceptual correction A-weighting curve)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Understanding\n",
    "*There are non-auditory factors involved in the understanding of speech by humans.  That is, the nature of speech structure and how that relates to understsanding, rather than the nature of human hearing and perception of speech which is a topic of psychoacoustic analysis.*\n",
    "\n",
    "### Intelligibility and Quality\n",
    "Intelligibility and Quality can be used interchangeably at times, but their measurement and dependencies are actually quite different.  *In very simple terms, quality is the measure of the fidelity of the speech.  This includes how well the speech under examination resembles some original speech, but extends beyond that how nice the speech sounds.  It is highly subjective measure but can be approximated objectively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement of Speech Intelligibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement of Speech Quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Toolkit\n",
    "\n",
    "### Zero-crossing rate ZCR\n",
    "It is a simple algorithm to determine the pitch that works well in the absence of noise.  It is designed to be a simple computational means to count how many times the speech signal crosses the zero axis for a window time period.  *The number of crossings per second will equal twice the frequency.  If we define sign { } to be a function returning +1 or 0 depending on whether the signal is greater than zero or not, then the ZCR for the ith analysis frame, of length N can be determined as*\n",
    "\n",
    "### Frame Power\n",
    "*This is the meaure of the signal energy over an analysis frame.  For speech frame i, with N elements, denoted by $x_i()$, the frame power meaure is determined from *\n",
    "$$ E_i=\\frac{1}{N}\\sum_{n=0}^{N-1}{|x_i(n)|^2}  - - - - \\dots(2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    function [fpow]=fpow(segment)\n",
    "        fpow=sum(segment.Ë†2)/length(segment);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average magnitude difference function\n",
    "\n",
    "*The average magnitude difference function is designed to provide much of the information of the frame powermeasure, but without multiplications*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Analysis and classification\n",
    "\n",
    "*The analysis of speech is an important requirement of many different applications and the classification of speech into various categories is a necessary part of many techniques.  The folloing ives basic techniques of applications of speech analysis and classification*\n",
    "\n",
    "1. detecting the presence of speech (VAD).\n",
    "2. detecting voiced and unvoiced speech\n",
    "3. finding boundaries between phonemes or words\n",
    "4. classifying speech by phoneme type\n",
    "5. language detection\n",
    "6. speaker recognition\n",
    "7. speech recognition.\n",
    "\n",
    "*Classification is an important, and growing area of speech research which relates to the machine 'understanding' of speech (where understanding can range from knowing whether a speech is present right through to understanding the meaning or emotion conveyed by the spoken text.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In order to begin such classification of speech, it is usually necessary to first perform some form of meaurement on the speech signal itself.  For example detecting voiced or unvoiced speech might require the determination of speech power and pitch, perhaps through examination of LSP data.  Many methods can potentially be used for analysis of speech, and extensive empirical testing is almost always required to determine the subset of meaures to be used for a particular application.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch Analysis\n",
    "\n",
    "### Joint time-frequency Distribution (JTFD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition Preprocessing Analysis\n",
    "\n",
    "*The main requirement for speech recogntion is the extraction of voice features, which may distinguish different phonemes of a language.  From a statistical point of view, this procedure is equivalent to finding a sofficient statistic to estimate phonemes*(Becchetti & Ricotti, 1998).  This has to be independent of the following other speech characteristics including\n",
    "1. phonatory apparatus\n",
    "2. speaker mood\n",
    "3. age\n",
    "4. sex\n",
    "5. dialect inflections and\n",
    "6. noise\n",
    "\n",
    "*To decrease vocal message ambiguity,* speech is therefore prefiltered before recognition. *Filtering is performed on discrete time quantised speech signals. Hence the first procedure consists of an analog to digital conversion.  Then the extraction procedure of the significant* speech features.  Cepstral analysis is used to demonstrate feature extraction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Features of speech signals\n",
    "*\n",
    "The frequency bandwidth of a speech signal is about 16kHz.  However, most of the speech energy is under 7kHz.  Speech bandwidth is generally reduced in recording.  A speech signal is called orthophonic if all the spectral components over 16kHz are discarded.  A telephonic lower quality signal is obtained whenever a signal does not have energy out of the band 300-3400Hz.  Therefore, digital speech processing is usually performed by frequency sampling ranging between 8 kHz and 32 kHz. These give a bandwidth of 4 kHz and 16 kHz respectively.\n",
    "*\n",
    "\n",
    "*\n",
    "Voice is produced by the phonatory mechanism articulators.  These are in a stable position for a very short time during the production of a phoneme, and then they move to a different stable position through an articulatory transition movement.  This is why a speech signal has a relevant variation each 80-200ms. \n",
    "*\n",
    "\n",
    "*\n",
    " A simple but effective mathematical model of the physiological voice production process is the excitation and vocal tract model.  The excitation represents the sound produced by the part of the phonatory physical system including the lungs and vocal cords, while the vocal tract is the duct through which the air passes to the mouth.  The excitation requires  a different mathematical description in the case of voiced or unvoiced sounds.\n",
    "*\n",
    "\n",
    "*\n",
    "The excitation  signal is assumed periodic with a period equal to the pitch for vowels and other voiced sounds, while for unvoiced consonants, the excitation is assumed to be white noise, i.e. a random signal without dominant frequencies.  The excitation signal is subject to spectral modifications while it passes through the vocal tract that has an acoustic effect equivalent to linear time invariant filtering.  These modifications contribute to the final sound characteristic features of different phonemes of a language.\n",
    "*\n",
    "\n",
    "*\n",
    "The model is significant because for each type of excitation, a phoneme is identified mainly by considering the shape of the vocal tract.  Therefore, the vocal tract configuration can be estimated by identify the filtering performed by the vocal tract on the excitation.  Introducing the power spectrum of the signal $P_x(\\omega)$, we have:\n",
    "*\n",
    "$$ P_x(\\omega)=P_v(\\omega)P_h(\\omega) - - - - \\dots (1) $$\n",
    "*\n",
    "Where $\\omega$ is the frequency of the discrete time signal.  \n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Processing\n",
    "*\n",
    "The characteristics of the vocal tract define the current uttered phoneme.  Such characteristics are evidenced in the frequency domain by the location of the formants i.e. the peaks given by resonances of the vocal tract.  Althrough possessing relevant information, high frequency formants have smaller amplitude with respect to low frequency formants.  A preemphasis of high frequencies is therefore required to obtain similar amplitude for all formants.  Such processing is usually obtained by filtering the speech signal with a first order  FIR filter whose transfer function is in the z-domain is:\n",
    "*\n",
    "$$ \\begin{aligned} H(z) =  1- a.z^{-1} & & \\text{for } 0\\le a \\le 1 - - - - \\dots (2) \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a being the preemphasis parameter.  In essence, in the time domain, the preemphasized signal is related to the input signal by the relation:*\n",
    "$$ x'(n)=x(n)-ax(n-1) $$\n",
    "*\n",
    "A typical value for a is 0.95, which gives rise to a more than 20 dB amplification of the high frequency spectrum.  Further pre-processing such as noise cancelling can be performed.  Finally, HMM-based ASR may experience a significant reduction in performance if temporally long silences are not removed from speech.  Since these silences should not be processed by ASR, effective speech detectors are required.  Simple detectors based on energy may be sufficient when the signal to noise ratio does not change appreciably.  \n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "\n",
    "*Traditional methods for spectral evaluation are reliable in the case of a stationary signal (i.e. a signal whose statistical characteristics are invariant with respect to time).  For voice, this holds only within the short time intervals of articulatory stability, during which a short time analysis can be performed by \"windowing\" a signal x'(n) into a succession of windowed sequences x<sub>t</sub>(n), t=1,2,...,T, called frames, which are then individually processed.*\n",
    "$$ \\begin{aligned}x'_t(n)=x'(n-t.Q), && 0\\le n\\le N, && 1\\le t\\le T \\end{aligned} - - - -\\dots(3)$$\n",
    "$$ x'(n)=w(n).x'_t(n) - - - -\\dots(4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*where w(n) is the impulse response of the window.  Each frame is shifted by a temporal length Q. If Q=N, frames do not temporarily overlap while if Q<N, N-Q samples at the end of a frame $x'_t(n)$ are duplicated at the beginning of the folloing frame  $x'_{t+1}(n)$.  We recall that Fourier analysis is performed through the Fourier transform that for a discrete signal  $x_t(n)$, is:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X_t(e^{j\\omega})=\\sum_{n=0}^{N-1}{x_t(n)e^{-j\\omega n}}=\\mathfrak{I}\\{x_t(n)\\} - - - -\\dots(5)$$\n",
    "\n",
    "*where $\\omega$ is the continuous frequency axis.  Introducing the Fourier transform of w(n) and $x'_t(n):W(e^{j\\omega})=\\mathfrak{I}\\{w(n)\\} X'_t(e^{j\\omega})=\\mathfrak{I}\\{x'_t(n)\\}$, a product in the time domain as in equation (4) becomes a convolution in the frequency domain.:*\n",
    "\n",
    "$$ X_t(e^{j\\omega})=\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}{X'_t(e^{j\\omega}).W(e^{j(\\omega-\\theta)}d\\theta=\\mathfrak{I}\\{x'_t(n)w(n)\\}} - -  - - \\dots(6) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering equations (3) and (4) equation (5) can be written as:\n",
    "$$ X_t(e^{j\\omega})=\\sum_{n=-\\infty}^{+\\infty}x'(n-t.Q).w(n)e^{-j\\omega n} - - - - \\dots(7) $$\n",
    "\n",
    "Equation (7) is also referred to as the Short Time Fourier Transform (STFT) or the Windowed Fourier Transform (WFT) of x'(n).\n",
    "\n",
    "The simplest window has a rectangular shape.  This window is implicitly used when a sequence of N samples is retrieved from a signal\n",
    "$$ w(n)=\\left\\{ \\begin{matrix} 1 & 0 \\le n \\le N-1 \n",
    "\\\\ 0 & otherwise \\end{matrix} \\right. - - - - \\dots(8) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The presence of a window provokes a distortion on the estimated spectrum since $X_t(e^{j\\omega})$ is the convolution of the spectrum of $x'_t(n)$ and of the Fourier transform of the rectangular window w(n).  $W(e^{j\\omega})$ is composed of a higher energy main lobe centred at the zero frequency and of lower energy side lobes centred at a higher frequencies.  The main lobe spreads out in a wider frequency range the narrow bnd power of the signal $x'_t(n)$ that in our case is represented by the formants.  This phenomenon reduces the local frequency resolution.  Moreover the side lobes of  $W(e^{j\\omega})$ swap energy from different and distant frequencies of $x'_t(n)$.  This problem is called leakage*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To reduce such effects $x_t(n)$ is multiplied by a properly shaped window w(n).  The choice of w(n) is a trade-off between several factors*\n",
    "- *the window shape may reduce distortion, but it may increase signal shape alteration*\n",
    "- *the length N is proportional to the frequency resolution and inversely proportional to the time resolution*\n",
    "- *the overlap N-Q is proportional to the frame rate, but it is also proportional to the corrleation of subsequent frames*\n",
    "\n",
    "*In ASR, the most-used window shape is the Hamming window, whose implulse response is raised cosine impulse:*\n",
    "$$ w(n)= \\left\\{\\begin{matrix} 0,54-0,46cos\\left(\\frac{2\\pi n}{N-1}\\right) & n=0, \\dots , N-1 \n",
    "\\\\ 0 & otherwise \\end{matrix} - - - - \\dots(9) \\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The side lobes of this window are much lower than the rectangular window (i.e. the leakage effect is decreased) although resolution is appreciably reduced.  This is because the Hamming main lobe is wider.*\n",
    "\n",
    "*The Hamming window is a good choice in speech recognition because high resolution is not required, considering that the next block in the feature extraction processing chain integrates all the closest frequency lines.  In contrast, leakage has a negative effect since vocal tract characteristics are obtained considering  the location and the amplitude of the peaks at distant frequencies.  Regarding the length N, widely used windows have 10-25 ms length.  The window length is chosen as a compromise solution between the required time and frequency resolution.  Short time (3-5ms) windows allow the detection of amplitude decay of formants, but have a too great impact on the frequency resolution that is required to estimate formant positions and therefore phoneme characterisation.  The length N of the window defines the spectral resolution of the Fourier representation.  Considering the sampling period $T_c=1/f_c$, it follows that by sampling the transformed sequence on the $\\omega$-axis at $2\\pi/N$ equally spaced points, the analog frequency resolution $\\delta f$ is:\n",
    "*\n",
    "$$ \\delta f=\\frac{\\delta\\omega}{2\\pi T_c}=\\frac{2\\pi}{2\\pi T_cN}=\\frac{f_c}{N} - - - -\\dots(10) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once sampling frequency f<sub>c</sub> is fixed, the spectral resolution is inversely proportional to the sequence length N.  A narrow-band spectrum is the one obtained when the resolution is high, while a wide-band is obtained when the resolution is low.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Increasing resolution is equivalent to using a longer sequence and this is in contrast to the requirement to analyze stationary signal segments.  A trade-off between these two requirements is necessary.  For instance, inthe case of f<sub>c</sub>=20kHz, the longest sequence compatible with stationarity should be composed of at most 512 samples (512/20=25.6ms), while the shortest one compatible with resolution should be composed of 64 samples (64/20=3.2ms)*\n",
    "\n",
    "*Moreover, larger windows (about 70 ms) have higher frequency resolution.  This allows identification of each single harmonic.  However, in such a case, fast transitions in the spectrum (as for instance the pronounciation of a stop consonants) are not detected.  Narrow windows have been proposed to estimate fast varying parameters of the vocal tract; while large windows are used to estimate fundamental frequency. A 20-30ms long window is generally a good compromise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Analysis\n",
    "*\n",
    "Standard methods of spectral analysis rely on the Fourier transform of $x_t(n):X_t(e^{j\\omega})$.  Computational complexity is greatly reduced if X_t(e^{j\\omega}) is evaluated only for a discrete number of $\\omega$ values.\n",
    "*\n",
    "\n",
    "*If such values are equally spaced, for instance considering $\\omega=2\\pi k/N$, then the Discrete Fourier Transform (DFT) of all frames of the signal is obtained*\n",
    "\n",
    "$$ X_t(k)=X_t(e^{j2\\pi k/N}) , k=0,\\dots,N-1 - - - -\\dots(11) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In addition, if the number of samples N is a power of 2, $N=2^n$ with p integer, the computational complexity can be additionally reduced to an order N log(N) resorting to the Fast Fourier algorithm (FFT). Note that if $x_t(n)$ is real, the FFT can be computed halving the computational complexity, which in this case is (N/2)log(N/2) (Press, Flannery, Teukolsky, & Vetterling, 1992).*\n",
    "\n",
    "*The characteristics of the vocal tract may be estimated by the periodogram of $x_t(n)$ that is simply the square magnitude of the DFT: $|X_t(k)|^2$ is an estimation of $P_x(\\omega)$ given in equation (11).*\n",
    "\n",
    "*\n",
    "Note that the phase information of the DFT samples of each frame is discarded.  This is consistent with the fact that phase does not carry useful information.  Perceptual experiments have proven that the perception of the signal reconstructed with random phases is almost indistinguishable from the original if the phase continuity betwen successive frames is preserved.\n",
    "*\n",
    "\n",
    "*Note also that the discrete STFT could be quite naturally enhanced by using the local cosine wavelets.*\n",
    "$$ \\{\\psi_{j,k}(n)\\}_{j\\in Z,k\\in N}=\\left\\{\\frac{\\sqrt{2}}{\\sqrt{\\left|I_j\\right|}}w_j(n)cos\\left[\\pi\\left(k+\\frac{1}{2}\\right)\\frac{n-a_j}{\\left|I_j\\right|}\\right]\\right\\}_{j\\in Z,k\\in N}\\dots(12) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "where $0 \\le n \\le \\left|I_j\\right|-1$ and $\\left\\{I_j\\right\\}_{j \\in Z}$ with |, the set of integers, forms an interval partition of the real line.  The scalar product of $\\psi_{j,k}(n)$ with the signal (being the preemphasis signal) is the Local Cosine Transform, whose windows, Malvar-Coifman-Meyer windows, are mostly adaptive.  The computational gain is mainly due to the fact that the above family forms and orthonormal basis of the space of the finite energy digitized signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter bank processing\n",
    "*\n",
    "Spectral analysis reveals those speech signal features which are mainly due to the shape of the vocal tract.  Spectral features of speech are generally obtained as the exit of filter banks, which properly integrate a spectrum at defined frequency ranges.  A set of 24 band-pass filters is generally used since it simulates ear processing\n",
    "*\n",
    "\n",
    "*\n",
    "Filters are non-uniformly spaced along the frequency axis.  As a rule, the part of the spectrum which is below 1kHz is processed by more filter banks since it contains more information on the vocal tract such as the first formant.  The frequency response of the filter banks simulates the perceptual processing within the ear and therefore such filtering is called **perceptual weighting**.\n",
    "*\n",
    "\n",
    "*Non-linear frequency analysis is also used to achieve frequency/time resolution.  Using narrow band-pass filters at low frequencies enables harmonics to be detected, but it gives poor onset information.  Using a longer bandwidth at higher frequencies allows for higher temporal resolution temporal resolution of bursts.*\n",
    "\n",
    "*In ASR, the most widely used perceptual scale in recognition is the Mel scale whose filter-bank characterisitcs are as follows.  The central frequency of each Mel filter bank is uniformly spaced before 1 kHz and it follows a logarithmic scale after 1kHz.  We recall that given the sampling period T<sub>c</sub> the frequency $\\omega$ of discrete time signals is related to the frequeny f of the respective continuous time signal by:*\n",
    "$$ f=\\frac{\\omega}{2\\pi T_c} - - - - \\dots(13) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are many methods to implement such filters.  A computationally inexpensive method consists of performing filtering directly in the DFT domain.  The DFT responses of the filters are simply shifted and frequency warped versions of a triangular window $U_{\\Delta_m} (k)$:*\n",
    "$$U_{\\Delta_m}(k)=\\left\\{\\begin{matrix}|k|<\\Delta_m \\rightarrow 1-|k|/\\Delta_m \\\\|k|\\ge\\Delta_m \\rightarrow 0 \\end{matrix} \\right. - - - -\\dots(14)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*where k is the DFT domain index, and $2\\Delta_m$ is the size of the m-th filter bank triangular window.  The m-th filter bank output is given by*\n",
    "$$Y_t(m)=\\sum_{k=b_m-\\Delta_m}^{b_m+\\Delta_m}X_t(k)U_{\\Delta_m}(k+b_m) - - - -\\dots(15) $$\n",
    "*where $X_t(k)$ is given by equation (11) and $1 \\le m \\le M$.  The central frequncy may be computed according to $b_m=b_{m-1}+\\Delta_m$, and, for $\\frac{\\omega}{2\\pi T_c}=f<1kHz, \\Delta_m$ is chosen so that 10 uniformly spaced filters are obtained.  For f>1kHz, the following approximation can be used: $\\Delta_m=1.2x\\Delta_{m-1}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log energy computation\n",
    "\n",
    "*The previous procedure has the role of smoothing the spectrum, performing processing that is similar to that executed by the human ear.  The next step consists of computing the logarithm of the square magnitude of the coefficients $Y_t(m)$ obtained from equation (15).  This reduces to simply computing the logarithm of the magnitude of the coefficients, because the logarithm algebraic property which brings back the logarithm of a power to a multiplication by a scaling factor.  Relevant benefits of this procedure can be seen by resorting to the frame work of cepstral analysis introduced in a subsequent section.  Here we note that magnitude and logarithm processing is performed by the ear as well.  Moreover, magnitude disgards the useless phase information while a logarithm performs a dynamic compression, making feature extraction less sensitive to variation in dynamics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Frequency cepstrum computation\n",
    "\n",
    "The final procedure for Mel frequency cepstrum computation (MFCC) consists of performing the inverse DFT on the logarithm of the magnitude of the filter bank output:\n",
    "$$y_t^{(m)}(k)=\\sum_{m=1}^Mlog\\left\\{\\left|Y_t(m)\\right|\\right\\}.cos\\left\\{k\\left(m-\\frac{1}{2}\\right)\\frac{\\pi}{M}\\right\\}, k=0,\\dots,L - - - - \\dots(16) $$\n",
    "\n",
    "The procedure has great advantages.  First, note that since the log power spectrum is real and symetric then the inverse DFT reduces to a Discrite Cosine Transform (DCT).  The DCT has the property to produce highly uncorrelated features $y_t^{(m)}(k)$.  Therefore, the stochastic characterisation of the feature process is simpler and in the probability density function of the features, generally modeled by linear combinations of Gaussian functions, diagonal covariance matrices can be used instead of full covariance matrices.   This significantly reduces the computational cost and the number of parameters to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero-order MFCC coefficient $y_t^{(0)}(k)$ is approximately equivalent to the log energy of the frame.  This coefficient is generally discarded since energy is directly computed on the time signal.  The DCT has also the effect of smoothing the spectrum if only the first coefficients are retained.  The number of MFCC coefficients is generally lower than 15 in  ASRs.  Referring to typical values are $1 \\le k < 9 or 1 \\le k < 12$ coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Coefficients and energy\n",
    "\n",
    "First note that cepstral coefficients are usualy joined to an energy coefficient $e_i$ taking into account the logarithm of the energy of the frame.  This parameter is useful since differenecs in energy are seen among different phonemes.\n",
    "\n",
    "A further improvement in performance is obtained by conidering that cepstral parameters and energy do not take into account the dynamic evolution of the speech signal, although such evolution carries releveant information for ASRs.  First- and second-order differences may be used to capture such information.  Hence, given a generic vector u, indexed in time t, the i-th order difference can be computed as\n",
    "$$ \\Delta'\\{u_t\\}=\\Delta^{t-1}\\{u_{t+1}\\}-\\Delta^{t-1}\\{u_{t-1}\\}, \\Delta^0\\{u_t\\}=u_t - - - - \\dots(17) $$\n",
    "\n",
    "Note that the i-th difference involves distant two time lags.  Higher or lower distances should be taken into account to time over-lapping.  Since too low distances may imply too correlated frames and therefore the dynamicis not caught by differences, higher values may imply frames describing too different states.  The vector features computed at time t may be composed of a set of L+1 acoustic features $\\{e_t, y_t^{(m)}(1),y_t^{(m)}(2),\\dots,y_t^{(m)}(L)\\}$, and their first- and second-order differences.\n",
    "$$ y_t=\\left\\{y_t^{(m)}(k)=e_t,\\Delta\\left\\{,y_t^{(m)}(k)\\right\\},\\Delta\\{e_t\\},\\Delta^2\\{,y_t^{(m)}(k)\\},\\Delta^2\\{e_t\\}\\right\\} - - - -\\dots(18) $$\n",
    "\n",
    "In modern ASR systems, L is set from 8 to 16 and first- and second- order derivatives are used in feature vector derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cepstrum Analysis\n",
    "*The complex cepstrum (the name is an anagram of spectrum) $\\hat{x}(n)$ for a discrete signal x(n) is the inverse Fourier transform of the complex logarithm $log X(e^{j\\omega})$*\n",
    "$$ \\hat{x}(n)=\\mathfrak{I}^{-1}\\{log X(e^{j\\omega})\\} - - - - \\dots(19) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The logarithm of the spectrum has the effect of reducing the component amplitudes at every frequency.  This logarithmic scale is also a characterisc of the human hearing system.  This logarithmic scale is also a characteristic of the human hearing system.  Therefore, those signals that are characterised by a combination of harmonics are better analyzed by the cepstrum rather than by the spectrum or the autocorrelation.*\n",
    "\n",
    "*The use of the cepstrum  was first introduced to discriminate voiced (vowels, sonorants) and unvoiced (plosives, affricates, etc.) speech segments.  In fact the cepstrum emphasises the formants of the vocal tract, even with noise.  In contrast the cepstrum is flat for sounds that lack a clear harmonic structure.  By exploiting these properties, the ceptstrum coefficients have been used to classify voice segments, determining an evolution of the cepstrum technique.  Inded cepstrum analysis, that is, an homomorphic analysis with a logarithm as the intermediate function, allows deconvolution of the speech signals as explained below.*\n",
    "\n",
    "*As already seen earlier, a speech waveform x(n) can be considered as a convolution between the excitation produced by the vocal cords v(n) and the impule response of a filter that represents the vocal tract h(n)*\n",
    "$$ x(n)=v(n)* h(n) - - - - \\dots(20) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the phonetic information is mainly related to the shape of the vocal tract, deconvolution algorithms for speech signal are of considerable interest to isolate the response of the vocal tract.  These algorithms belong to the system theory branch known as homomorphic filtering.  Resulting to the complex cepstrum we have:*\n",
    "$$ \\begin{aligned} \\hat{x}(n) &=\\mathfrak{I}^{-1}\\left\\{log(\\mathfrak{I}[v(n)*h(n)])\\right\\} \\\\\n",
    " &=\\mathfrak{I}^{-1}\\left\\{log(V(e^{j\\omega}))+log(H(e^{j\\omega}))\\right\\}=\\hat{v}(n)+\\hat{h}(n) \\end{aligned}- - - - \\dots(21) $$\n",
    "*where $\\hat{v}(n)\\text{, }\\hat{h}(n)$ are the complex cepstra of v(n) and h(n) respectively.  The complex cepstrum transforms the convolution into a sum of two components $\\hat{v}(n)\\text{, }\\hat{h}(n)$ that can be separated by band pass linear filters, if there is no frequency overlapping*\n",
    "\n",
    "*For speech signals, this is feasible since the short time spectrum shows that the envolope of the vocal tract filter h(n) change slowly with repsect to the fine structure of the harmonics that are produced by the periodic excitation of speech v(n).*\n",
    "\n",
    "*For minimum phase signals or when phase information is not of interest the real ceptstrum $\\hat{x}_r(n)$ can be used instead of the complex cepstrum.  The real cepstrum of a signal is defined by the inverse Fourier transform of the logarithm of the magnitude of $X(e^{j\\omega})$*\n",
    "\n",
    "\n",
    "$$ \\hat{x}_r(n)=\\mathfrak{I}^{-1}\\{log \\left|X(e^{j\\omega})\\right|\\} - - - - \\dots(22) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The real cepstrum can be computed using the inverse DFT.  The scheme recalls that used to compute the MFCC.  The relevant difference beteen the two schemes is that for MFCC, the cepstrum is computed on a perceptually-weighted spectrum obtain from the filter banks.\n",
    "*\n",
    "\n",
    "*Homomorphic deconvolution expressed in equation (21) may highlight relevant properties of MFCC.  First, not that if a multiplicative constant applied to a speech signal, the logarithm of such a constant is added to all the coefficients of $log\\left|Y_t(m)\\right|^2$.  Such a constant influences only the zero coefficient $y_t^{(m)}(0)$ of the MFCC.  Therefore, MFCC are not sensitive to a gain factor, apart from $y_t^{(m)}(0)$.  Note also that vocal tract response and signal excitation are combined additively in cepstrum as shown in equation (21).  The vocal tract log spectrum has a smooth behavior while excitation has a highly variant quasi periodic spectrum for voiced sounds.  Thus, vocal tract response can be obtained by simply retaining the first cepstral coefficients $y_t^{(m)}(k)$.   That is why only k-th coefficients, $k \\le L \\le 15$, are retained.  Note also that environment influence can be modeled as a linear filter.  This degradation becomes a bias on the log spectrum estimation that can be evaluated and removed.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Becchetti, C., & Ricotti, L. P. (1998). Speech recognition: theory and C++ implementation. New York: Wiley. \n",
    "\n",
    "McLoughlin, I. (2009). Applied speech and audio processing: With Matlab examples. Cambridge: Cambridge University Press.\n",
    "\n",
    "Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1990). Numerical recipes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
