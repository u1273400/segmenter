{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Probability\n",
    "\n",
    "\n",
    "### Basic Properties\n",
    "Let $(\\Omega,\\mathcal{A},P)$ be a probability measure with $E, F, E^c \\in \\mathcal{A}$\n",
    "1. $P(E\\cup F)=P(E)+P(F)$ if $E \\cap F=0$\n",
    "2. $P(E\\cup F)=P(E)+P(F)-P(E\\cap F)$\n",
    "3. $P(E)=1-P(E^c)$\n",
    "4. $P(E \\cup F^c)=P(E)-P(E\\cap F)$\n",
    "5. **Inclusion-Exclusion Formula** $$P(\\bigcup_{i=1}^nE_i)=\\sum_iP(E_i)-\\sum_{i<j}P(E_i\\cap E_j)+\\sum_{i<j<k}P(E_i\\cap E_j \\cap E_k) + \\dots\\ +(-1)^{n+1}P(E_1\\cap E_2\\cap\\dots\\cap E_n)$$\n",
    "![Figure 1](https://selene.hud.ac.uk/u1273400/images/seg_media/ierule.PNG)\n",
    "**Figure 1: Inclusion Exclusion Rule**\n",
    "6. $P(\\bigcup_{i=1}^nE_i)\\le \\sum_{i=1}^nP(E_i)$ and $P(\\bigcup_{i=1}^\\infty E_i)\\le\\sum_{i=1}^\\infty P(E_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "![Figure 1](https://selene.hud.ac.uk/u1273400/images/seg_media/cprob.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manning Statistical NLP Outline\n",
    "- Part I lays out the mathematical and linguistic foundation that the other parts build on. Concepts and techniques introduced here are referred to throughout the book.\n",
    "- Part II covers word-centered work in Statistical NLP. There is a natural progression from simple to complex linguistic phenomena in its four chapters on collocations, n-gram models, word sense disambiguation, and lexical acquisition, but each chapter can also be read on its own.  \n",
    "- The four chapters in part III, Markov Models, tagging, probabilistic context free grammars, and probabilistic parsing, build on each other, and so they are best presented in sequence. However, the tagging chapter can be read separately with occasional references to the Markov Model chapter. \n",
    "- The topics of part IV are four applications and techniques: statistical alignment and machine translation, clustering, information retrieval, and text categorization. Again, these chapters can be treated separately according to interests and time available, with the few dependencies between them marked appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Space\n",
    "\n",
    "- Probability theory deals with predicting how likely it is that something will happen. For example, if one tosses three coins, how likely is it that they will all come up heads? \n",
    "- The notion of the **likelihood** of something is formalized through the concept of an **experiment (or trial)** - the process by which an observation is made. In this technical sense, tossing three coins is an experiment.\n",
    "- All that is crucial is that the experimental protocol is well defined. We assume a collection of **basic outcomes (or sample points)** for our experiment,\n",
    "the **sample space $\\Omega$**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample spaces may either be **discrete**, having at most a countable or finite number of basic outcomes, or **continuous**, having an uncountable/infinite number of basic outcomes (for example, measuring a personâ€™s height). \n",
    "- For most language applications, the sample spaces will be discrete sample. \n",
    "- An **event** A is a subset of $\\Omega$ if, for example, in the coin experiment, the first coin being a head, and the second and third coming down tails is one basic outcome, while any result of one head and two tails is an example of an event. \n",
    "- Note also that $\\Omega$ represents the certain event, the space of all possible experimental outcomes, and 0 represents the impossible event. \n",
    "- We say that an experimental outcome must be an event. \n",
    "- The foundations of probability theory depend on the set of events $\\mathcal{F}$ forming a **$\\sigma-field$ **- a set with a maximal element $\\Omega$ and arbitrary complements and unions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These requirements are trivially satisfied by making the set of events, the **event space**, the power set of the sample space (that is, the set of all subsets of the sample space, often written $2^\\mathcal{F}$).\n",
    "- Probabilities are numbers between O and 1, where O indicates impossibility and 1 certainty. \n",
    "- A **probability function** (also known as a probability distribution) distributes a probability mass of 1 throughout the sample space 0. \n",
    "- Formally, a discrete probability function is any function\n",
    "$$ P{\\Omega}=1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countable additivity \n",
    "- For **disjoint sets** $A_j \\in \\mathcal{F} (i.e., A_j \\cap A_k = 0\\text{ for }j \\ne k)$. \n",
    "$$P\\left(\\bigcup_{j=1}^\\infty A_j\\right)=\\sum_{j=1}^\\infty P(A_j) - - - -\\dots(1)$$\n",
    "- We call P(A) the probability of the event A.  \n",
    "- These axioms say that an event that encompasses, say, three distinct possibilities must have a probability that is the sum of the probabilities of each possibility, and that since an experiment must have some basic outcome as its result, the probability of that is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Space\n",
    "A well founded probability space consists of a sample space $\\Omega$, a $\\sigma-field$ of events $\\mathcal{F}$, and a probability function P.  \n",
    "\n",
    "##### Example 1\n",
    "A fair coin is tossed 3 times.  What is the chance of 2 heads?\n",
    "\n",
    "##### Solution\n",
    "$$ \\Omega = {HHH, HHT, HTH, HTT,THH,THT, TTH, TTT}$$\n",
    "Each of the basic outcomes in $\\Omega$ is equally likely, and thus has probability 1/8.  A situation where each basic outcome is equally likely is called a *uniform distribution*.  In a finite sample space with equiprobable basic outcomes, $P(A)=\\frac{\\left|A\\right|}{\\left|\\Omega\\right|}$ (where |A| is the number of elements within set A).  The event of interest is\n",
    "$$ A = {HHT, HTH, THH}$$\n",
    "So\n",
    "$$ P(A)=\\frac{\\left|A\\right|}{\\left|\\Omega\\right|}=\\frac{3}{8} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability and Independence\n",
    "\n",
    "- Sometimes we have partial knowledge about the outcome of an experiment and that naturally influences what experimental outcomes are possible.\n",
    "- We capture this knowledge through the notion of **conditional probability**. This is the updated probability of an event given some knowledge.\n",
    "- The probability of an event before we consider our additional knowledge is called the **prior probability** of the event, while the new probability that results from using our additional knowledge is referred to as the **posterior probability** of the event. \n",
    "- The conditional probability of an event A given that an event B has occurred (P(B)>0) is\n",
    "$$ P(A|B)=\\frac{P(A\\cap B)}{P(B)}$$\n",
    "Even if P(B)=0 we have that\n",
    "$$ P(A\\cap B)=P(B)P(A|B)=P(A)P(B|A) - - - -\\dots(2) $$\n",
    "The generalisation of this rule to multiple events is the central **chain rule**:\n",
    "$$P(A_1\\cap\\dots\\cap A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1\\cap A_2)\\dots P(A_n|\\cap_{i=1}^{n-1}A_i) - - \\dots(3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two events A, B are **independent** of each other if P(AnB) = P(A)P(B).\n",
    "- Unless P(B) = 0 this is equivalent to saying that P(A) = P(A|B) (i.e., knowing that B is the case does not affect the probability of A). \n",
    "- This equivalence follows trivially from the chain rule. \n",
    "- Otherwise events are **dependent**. \n",
    "- We can also say that A and B are conditionally independent given C when P(AnB|C) = P(A|C)P(B|C)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
