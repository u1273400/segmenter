{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory\n",
    "\n",
    "- The field of information theory was developed in the 1940s by Claude Shannon, with the initial exposition reported in (Shannon 1948). \n",
    "- Shannon was interested in the problem of maximizing the amount of information that you can transmit over an imperfect communication channel such as a noisy phone line (though actually many of his concerns stemmed from codebreaking in World War II). \n",
    "- For any source of ‘information’ and any ‘communication channel,’ Shannon wanted to be able to determine theoretical maxima for (i) data compression - which turns out to be given by the Entropy H (or more fundamentally, by the Kolmogorov complexity K), and (ii) the transmission rate - which is given by the Channel Capacity C. \n",
    "- Until Shannon, people had assumed that necessarily, if you send your message at a higher speed, then more errors must occur during the transmission. But Shannon showed that providing that you transmit the information in your message at a slower rate than the Channel Capacity, then you can make the probability of errors in the transmission of your message as small as you would like.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "- Let p(x) be the probability mass function of a random variable X, over a discrete set of symbols (or **alphabet**) $mathcal{X}$:\n",
    "- $p(x)=P(X=x), x \\in \\mathcal{X}$\n",
    "- For example, if we toss two coins and count the number of heads, we have a random variable: p(0)=1/4, p(1)=1/2, p(2)=1/4.\n",
    "- The **entropy** (or **self-information**) is the average uncertainty of a single random variable:\n",
    "$$ Entropy\\text{ }H(p)=H(X)=-\\sum_{x\\in\\mathcal{X}}p(x)log_2p(x) - - - -\\dots(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy measures the amount of information in a random variable. \n",
    "- It is normally measured in bits (hence the log to the base 2), but using any other base yields only a linear scaling of results. \n",
    "- For the rest of this book, an unadorned log should be read as log to the base 2. \n",
    "- Also, for this definition to make sense, we define 0 log 0 = 0.\n",
    "\n",
    "#### Example 1\n",
    "Suppose you are reporting the result of rolling an g-sided die. Then the entropy is:\n",
    "\n",
    "$$H(X)=-\\sum_{i=1}^8p(i)log{p}(i)=-\\sum_{i=1}^8\\frac{1}{8}log{\\frac{1}{8}}= -log{\\frac{1}{8}}=log8=3 bits$$.\n",
    "\n",
    "- This is what we would expect. Entropy, the amount of information in a random variable, can be thought of as the average length of the message needed to transmit an outcome of that variable. \n",
    "- If we wish to send the result of rolling an eight-sided die, the most efficient way is to simply encode the result as a 3 digit binary message:\n",
    "$$\\begin{matrix}1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\\\n",
    "001 & 010 & 011 & 100 & 101 & 110 & 111 & 000 \\end{matrix}$$\n",
    "\n",
    "- The transmission cost of each result is 3 bits, and there is no cleverer way of encoding the results with a lower average transmission cost. \n",
    "- In general, an optimal code sends a message of probability p(i) in I- log p(i) 1 bits.\n",
    "- The minus sign at the start of the formula for entropy can be moved inside the logarithm, where it becomes a reciprocal\n",
    "$$H(X)=\\sum_{x\\in\\mathcal{X}}p(x)log{\\frac{1}{p(x)}} - - - -\\dots(2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- People without any statistics background often think about a formula like this as a sum of the quantity p(x) log( l/ p(x)) for each x.\n",
    "- While this is mathematically impeccable, it is the wrong way to think about such equations. Rather you should think of $\\sum_{x\\in\\mathcal{X}}p(x)\\dots$ as an idiom. \n",
    "- It says to take a weighted average of the rest of the formula (which will be a function of x), where the weighting depends on the probability of each x.\n",
    "- Technically, this idiom defines an expectation, as we saw earlier. Indeed\n",
    "$$H(X)=E\\left(log\\frac{1}{p(X)}\\right) - - - -\\dots(3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Simplified Polynesian\n",
    "Simplified Polynesian (languages such as Hawai'ian known for small alphabets) appears to be just a random sequence of letters, with the letter frequencies as shown:\n",
    "Then the per-letter entropy is:\n",
    "$$\\begin{aligned}H(P) &= - \\sum_{i\\in\\{p,t,k,a,i,u\\}}P(i)log{P(i)} \\\\\n",
    "    &= -[4\\times\\frac{1}{8}log{\\frac{1}{8}}+2\\times\\frac{1}{4}log{\\frac{1}{4}}] \\\\\n",
    "    &= 2\\frac{1}{2} bits\n",
    "\\end{aligned}$$\n",
    "\n",
    "- This is supported by the fact that we can design a code that on average takes $2\\frac{1}{2}$ bits to transmit a letter:\n",
    "\n",
    "$$\\begin{matrix}p & t & k & a & i & u \\\\\n",
    "100 & 00 & 101 & 01 & 110 & 111 \\end{matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that this code has been designed so that fewer bits are used to send more frequent letters, but still so that it can be unambiguously decoded\n",
    "- if a code starts with a 0 then it is of length two, and if it starts with a 1 it is of length 3. \n",
    "- There is much work in information theory on the design of such codes, but we will not further discuss them here.\n",
    "- One can also think of entropy in terms of the Twenty Questions game.\n",
    "- If you can ask yes/no questions like ‘Is it a t or an a?’ or ‘Is it a consonant?’ then on average you will need to ask $2\\frac{1}{2}$ questions to identify each letter with total certainty (assuming that you ask good questions!).\n",
    "- In other words, entropy can be interpreted as a measure of the size of the ‘search space’ consisting of the possible values of a random variable and its associated probabilities.\n",
    "- Note that: (i) $H(X)\\ge 0$, (ii) H(X) = 0 only when the value of X is determinate, hence providing no new information, and that (iii) entropy increases with the message length. \n",
    "- The information needed to transmit the results of tossing a possibly weighted coin depends on the probability p that it comes up heads, and on the number of tosses made. \n",
    "- The entropy for a single toss is shown in figure 1. \n",
    "- For multiple tosses, since each is independent, we would just multiply the number in the graph by the number of tosses.\n",
    "![Figure 1](https://selene.hud.ac.uk/u1273400/images/seg_media/ent0.PNG)\n",
    "**Figure 1**:Theentropy of a weighted coin. The horizontal axis shows the probability of a weighted coin to come up heads. The vertical axis shows the entropy of tossing the corresponding coin once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint entropy and conditional entropy\n",
    "- The joint entropy of a pair of discrete random variables X, Y ~p(x,y) is the amount of information needed on average to specify both their values. It is defined as\n",
    "$$ H(X,Y)=-\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}p(x,y)log{p(x,y)} - - - -\\dots(4)$$\n",
    "- The conditional entropy of a discrete random variable Y given another X, for X, Y - p(x, y), expresses how much extra information you still need to supply on average to communicate Y given that the other party knows X:\n",
    "$$\\begin{aligned}H(Y|X) &=\\sum_{x\\in\\mathcal{X}}p(x)H(Y|X=x) \\\\\n",
    "    &= \\sum_{x\\in\\mathcal{X}}p(x)\\left[-sum_{y\\in\\mathcal{Y}}log{p(y|x)}\\right] \\\\\n",
    "    &= -\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}p(x,y)log{p(y|x)}\n",
    "\\end{aligned} - - - -\\dots(5)$$\n",
    "There is also a chain rule for entropy:\n",
    "$$\\begin{aligned}H(X,Y) &= H(X)+H(Y|X) \\\\\n",
    "    H(X_1,\\dots,X_n) &= H(X)+H(X_2|X_1)+\\dots+H(X_n|X_1,\\dots,X_{n-1}\n",
    "\\end{aligned} - - - -\\dots(6)$$\n",
    "\n",
    "- The products in the chain rules for probabilities here become sums because of the log\n",
    "$$\\begin{aligned} H(X,Y) &= -E_{p(x,y)}(log{p}(x,y)) \\\\\n",
    "    &= -E_{p(x,y)}(log{p}(x)p(y|x))) \\\\\n",
    "    &= -E_{p(x,y)}(log{p}(x)+log{p}(y|x))) \\\\\n",
    "    &= -E_{p(x,y)}(log{p}(x))-E_{p(x,y)}(log{p}(y|x)) \\\\\n",
    "    &= H(X)+H(Y|X)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Simplified Polynesian Revisited\n",
    "- An important scientific idea is the distinction between a model and reality. Simplified Polynesian isn’t a random variable, but we approximated it (or modeled it) as one. But now let’s learn a bit more about the language. Further fieldwork has revealed that Simplified Polynesian has syllable structure. Indeed, it turns out that all words consist of sequences of CV (consonant-vowel) syllables. This suggests a better model in terms of two random variables C for the consonant of a syllable, and V for the vowel, whose joint distribution P(C, V) and marginal distributions P( C, .) and P(. , V) are as follows:\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-yw4l\"></th>\n",
    "    <th class=\"tg-yw4l\">p</th>\n",
    "    <th class=\"tg-yw4l\">t</th>\n",
    "    <th class=\"tg-yw4l\">k</th>\n",
    "    <th class=\"tg-yw4l\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">a</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{16}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{3}{8}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{16}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{2}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">i</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{16}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{3}{16}$</td>\n",
    "    <td class=\"tg-yw4l\">0</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{4}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">u</td>\n",
    "    <td class=\"tg-yw4l\">0</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{3}{16}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{16}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{4}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\"></td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{8}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{3}{4}$</td>\n",
    "    <td class=\"tg-yw4l\">$\\frac{1}{16}$</td>\n",
    "    <td class=\"tg-yw4l\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that here the marginal probabilities are on a per-syllable basis, and are therefore double the probabilities of the letters on a per-letter basis, which would be:\n",
    "$$\\begin{matrix}p & t & k & a & i & u \\\\\n",
    "1/16 & 3/8 & 1/16 & 1/4 & 1/8 & 1/8 \\end{matrix}$$\n",
    "\n",
    "##### Solution\n",
    "\n",
    "- We can work out the entropy of the joint distribution, in more than one way.  Let us use the chain rule.\n",
    "- Within the calculation, we use an informal, but convenient, notation of expressing a finite-valued distribution as a sequence of probabilities, which we can calculate the entropy of.\n",
    "\n",
    "$$\\begin{aligned}H(C) &= 2 \\times \\frac{1}{8} \\times 3 + \\frac{3}{4}(2-log{3}) \\\\\n",
    "    &= \\frac{9}{4}-\\frac{3}{4}log{3}\\text{ bits }\\approx 1.061 bits\n",
    "\\end{aligned}$$\n",
    "$$\\begin{aligned}H(V|C) &= \\sum_{c=p,t,k}p(\\mathcal{C}=c)H(V|\\mathcal{C}=c) \\\\\n",
    "    &= \\frac{1}{8}H\\left(\\frac{1}{2},\\frac{1}{2},0\\right)-\\frac{3}{4}H\\left(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4}\\right)+\\frac{1}{8}H\\left(\\frac{1}{2},0\\frac{1}{2}\\right) \\\\\n",
    "    &= 2\\times\\frac{1}{8}\\times 1+\\frac{3}{4}\\left[\\frac{1}{2}\\times 1+2\\times\\frac{1}{4}\\times 2\\right] \\\\\n",
    "    &= \\frac{1}{4} + \\frac{3}{4} \\times \\frac{3}{2} \\\\\n",
    "    &= \\frac{11}{8} bits = 1.375 bits\n",
    "\\end{aligned}$$\n",
    "$$\\begin{aligned}H(C,V) &=H(C)+H(V|C) \\\\\n",
    "    &= \\frac{9}{4}-\\frac{3}{4}log{3}+\\frac{11}{8} \\\\\n",
    "    &= \\frac{29}{8}-\\frac{3}{4}log{3}\\text{ bits }\\approx 2.44 bits\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that those 2.44 bits are now the entropy for a whole syllable (which was $2 x 2\\frac{1}{2} = 5$ for the original Simplified Polynesian example). \n",
    "- Our better understanding of the language means that we are now much less uncertain, and hence less surprised by what we see on average than before.\n",
    "- Because the amount of information contained in a message depends on the length of the message, we normally want to talk in terms of the perletter or per-word entropy. \n",
    "- For a message of length n, the per-letter/word entropy, also known as the entropy rate, is:\n",
    "$$H_{rate}=\\frac{1}{n}H(X_{1n})=-\\frac{1}{n}\\sum_{x_{1n}}(x_{1n})log{p}(x_{1n}) - - - -\\dots(7)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we then assume that a language is a stochastic process consisting of a sequence of tokens $L=(Xi)$, for example a transcription of every word you utter in your life, or a corpus comprising everything that is sent down the newswire to your local paper, then we can define the entropy of a human language L as the entropy rate for that stochastic process:\n",
    "$$H_{rate}(L)=\\lim_{n\\rightarrow\\infty}\\frac{1}{n}H(X_1,X_2,\\dots,X_n)- - - -\\dots(8)$$\n",
    "- We take the entropy rate of a language to be the limit of the entropy rate of a sample of the language as the sample gets longer and longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information\n",
    "By the chain rule for entropy,\n",
    "$$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$\n",
    "Therefore\n",
    "$$H(X)-H(X|Y)=H(Y)-H(Y|X)$$\n",
    "\n",
    "- This difference is called the mutual information between X and Y. \n",
    "- It is the reduction in uncertainty of one random variable due to knowing about another, or in other words, the amount of information one random variable contains about another. \n",
    "- A diagram illustrating the definition of mutual information and its relationship to entropy is shown in figure 2.\n",
    "![Figure 2](https://selene.hud.ac.uk/u1273400/images/seg_media/ent1.PNG)\n",
    "**Figure 2**:The relationship between mutual information I and entropy H."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual information is a symmetric, non-negative measure of the common information in the two variables. People thus often think of mutual information as a measure of dependence between variables. However, it is actually better to think of it as a measure of independence because:\n",
    "- It is 0 only when two variables are independent, but\n",
    "- For two dependent variables, mutual information grows not only with the degree of dependence, but also according to the entropy of the variables.\n",
    "- Simple arithmetic gives us the following formulas for mutual information I(X;Y):\n",
    "$$\\begin{aligned}I(X;Y) &=H(X)-H(X|Y) \\\\\n",
    "    &= H(X)+H(Y)-H(X,Y) \\\\\n",
    "    &= \\sum_{x}P(x)log\\frac{1}{p(x)}+\\sum_yp(y)log\\frac{1}{p(y)}+\\sum_{x,y}p(x,y)log{p}(x,y) \\\\\n",
    "    &= \\sum_{x,y}log\\frac{p(x,y)}{p(x)p(y)}\n",
    "\\end{aligned}- -(9)$$\n",
    "\n",
    "- Since H(X|X)=0 Note that H(X)=H(X)-H(X|X)=I(X;X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This illustrates both why entropy is also called self-information, and how the mutual information between two totally dependent variables is not constant but depends on their entropy.\n",
    "- We can also derive conditional mutual information and a chain rule:\n",
    "$$ I(X;Y|Z)=I(X;Y|Z)=H(X|Z)-H((X|Y,Z) - - - -\\dots(10)$$\n",
    "$$\\begin{aligned}I(X_{1n};Y) &=I(X_1;Y)+\\dots+ I(X_n;Y|X_1,\\dots,X_{n-1}) \\\\\n",
    "    &= \\sum_{i=1}^nI(X_i;Y|X_1,\\dots,X_{n-1})\n",
    "\\end{aligned}- -(11)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we have defined the mutual information between two  random variables. Sometimes people talk about the **pointwise mutual information** between two particular points in those distributions\n",
    "$$I(x,y)=log\\frac{p(x,Y)}{p(x)P(Y)}$$\n",
    "\n",
    "- This has sometimes been used as a measure of association between elements, but there are problems with using this measure\n",
    "- Mutual information has been used many times in Statistical NLP, such as for clustering words \n",
    "- It also turns up in word sense disambiguation.\n",
    "\n",
    "### The noisy channel\n",
    "- Using information theory, Shannon modeled the goal of communicating down a telephone line - or in general across any channel - in the following way: \n",
    "- The aim is to optimize in terms of throughput and accuracy the communication of messages in the presence of noise in the channel. \n",
    "- It is assumed that the output of the channel depends probabilistically on  the input.\n",
    "- In general, there is a duality between **compression**, which is achieved by removing all redundancy, and transmission accuracy, which is achieved by adding controlled **redundancy** so that the input can be recovered even in the presence of noise. \n",
    "- The goal is to encode the message in such a way that it occupies minimal space while still containing enough redundancy to be able to detect and correct errors. \n",
    "- On receipt, the message is then decoded to give what was most likely the original message. This process is shown in figure 3.\n",
    "![Figure 3](https://selene.hud.ac.uk/u1273400/images/seg_media/ent2.PNG)\n",
    "**Figure 3**: The noisy channel model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The central concept that characterizes a channel in information theory is its **capacity**. \n",
    "- The channel capacity describes the rate at which one can transmit information through the channel with an arbitrarily low probability of being unable to recover the input from the output. \n",
    "- For a memoryless channel, Shannon’s second theorem states that the channel capacity can be determined in terms of mutual information as follow\n",
    "$$ C=max_{p(X)}I(X;Y) - - - -\\dots(13)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to this definition, we reach a channel’s capacity if we manage to design an input code X whose distribution maximizes the mutual information between the input and the output over all possible input distributions p(X).\n",
    "- As an example, consider the binary symmetric channel in figure 4.\n",
    "![Figure 4](https://selene.hud.ac.uk/u1273400/images/seg_media/ent3.PNG)\n",
    "**Figure 4** A binary symmetric channel. A 1 or a 0 in the input gets flipped on trasnsmission with probability p\n",
    "\n",
    "- Each input symbol is either a 1 or a 0, and noise in the channel causes each symbol to be flipped in the output with probability p. We find that\n",
    "$$ I(X;Y)=H(Y)-H(Y|X) = H(Y)-H(p)$$\n",
    "\n",
    "Therefore\n",
    "$$max_{p(X)}I(X;Y)=1-H(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This last line follows because the mutual information is maximized by maximizing the entropy in the codes, which is done by making the input and hence the output distribution uniform, so their entropy is 1 bit.\n",
    "- Since entropy is non-negative, $C \\le 1$. \n",
    "- The channel capacity is 1 bit only if the entropy is zero, that is if p = 0 and the channel reliably transmits a 0 as 0 and a 1 as 1, or if p = 1 and it always flips bits. \n",
    "- A completely noisy binary channel which transmits both OS and 1s with equal probability as OS and 1s (i.e., p = i) has capacity C = 0, since in this case there is no mutual information between X and Y.\n",
    "- Such a channel is useless for communication.\n",
    "- It was one of the early triumphs of information theory that Shannon was able to show two important properties of channels. \n",
    "- First, channel capacity is a well-defined notion. In other words, for each channel there is a smallest upper bound of I(X; Y) over possible distributions p(X).\n",
    "- Second, in many practical applications it is easy to get close to the optimal channel capacity.\n",
    "- We can design a code appropriate for the channel that will transmit information at a rate that is optimal or very close to optimal.\n",
    "- The concept of capacity eliminates a good part of the guesswork that was involved in designing communications systems before Shannon.\n",
    "- One can precisely evaluate how good a code is for a communication line and design systems with optimal or near-optimal performance.\n",
    "- The noisy channel model is important in Statistical NLP because a simplified version of it was at the heart of the renaissance of quantitative natural language processing in the 1970s. \n",
    "- In the first large quantitative project after the early quantitative NLP work in the 1950s and 60s researchers at IBM’s T. J. Watson research center cast both speech recognition and machine translation as a noisy channel problem.\n",
    "- Doing linguistics via the noisy channel model, we do not get to control the encoding phase. We simply want to decode the output to give the most likely input, and so we work with the channel shown in figure 5. \n",
    "![Figure 5](https://selene.hud.ac.uk/u1273400/images/seg_media/ent4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many problems in NLP can be construed as an attempt to determine the most likely input given a certain output. \n",
    "- We can determine this as follows, by using Bayes' theorem, and then noting that the output probability is constant:\n",
    "$$ \\hat{I}=argmax_Ip(i|o)=argmax_I\\frac{p(i)p(o|i)}{p(o)}=argmax_Ip(i)p(o|i) - - - - \\dots(14)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two probability distributions to consider: p(i) is the language\n",
    "model, the distribution of sequences of ‘words’ in the input lan-\n",
    "CHANNEL guage, and p( o I i) is the channel probability.\n",
    "PROBABILITY As an example, suppose we want to translate a text from English to\n",
    "French. The noisy channel model for translation assumes that the true\n",
    "text is in French, but that, unfortunately, when it was transmitted to us,\n",
    "it went through a noisy communication channel and came out as English.\n",
    "So the word cow we see in the text was really vuche, garbled by the noisy\n",
    "channel to cow. All we need to do in order to translate is to recover the\n",
    "DECODE original French - or to decode the English to get the French.7\n",
    "The validity of the noisy channel model for translation is still giving\n",
    "rise to many a heated debate among NLP researchers, but there is no\n",
    "doubt that it is an elegant mathematical framework that has inspired a\n",
    "significant amount of important research. We will discuss the model in\n",
    "more detail in chapter 13. Other problems in Statistical NLP can also be\n",
    "seen as instantiations of the decoding problem. A selection is shown in\n",
    "table 2.2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
