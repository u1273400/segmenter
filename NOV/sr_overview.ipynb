{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition Overview\n",
    "\n",
    "### Introduction\n",
    "\n",
    "*We can classfify speech recognition tasks and systems along a set of dimensions that produce various tradeoffs in applicability and robustness.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolated word versus contiuous speech\n",
    "\n",
    "*Some speech systems only need identify single words at a time (e.g., speaking a number to route a phone call to a company to the appropriate person), while others must recognise sequences of words at a time.  The isolated word systems, are not surprisingly, easier to construct and can be quite robust as they have a complete set of patterns for the possible inputs.  Continuous word systems cannot have complete representation s of all possible inputs, but must assemble patterns of smaller speech events (e.g., words) into larger sequences (e.g. sentences)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speaker Dependent versus speaker independent systems\n",
    "\n",
    "*A speaker dependent system is a system where the speech patterns are constructed (or adapted) to a single speaker.  Speaker independent systems must handle a wide range of speakers.  Speaker dependent systems are more accurate, but the training is not feasible in many applications.  For instance, an automated telephone or operator system must handle any person that calls in, and cannot ask the person to go through a training phase before using the system.  With a dictation system on your personal computer, on the other hand, it is feasible to ak the user to perform a hour or so of training in order to build a recognition model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small versus vocabulary systems\n",
    "*Small vocabulary systems are typically less than 100 words (e.g. speech interface for long distance dialing), and it is possible to get quite accurate recognition for a wide range of users.  Large vocabulary systems e.g. say 20,000 words or greater), typically need to be speaker dependent to get good accuracy (at least for systems that recognise in real time).  Finally, there are mid-size systems, on the order of 1000-3000 words, which are typical sizes for current research-based spoken dialog systems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Some applications can make restrictive assumption possible.  For instance, voice dialing on cell phones has a small vocabulary (less than 100 names), is speaker dependent (the user says every word that needs to be recognized a couple of times to train it), and isolated word.  On the other extreme, there are research systems that attempt to trascribe recordings of meetings among several people.  These must handle speaker independent, continuous speech, with large vocabularies.  At present, the best research systems cannot achieeve must better than a 50% recognition rate, even with fairly high quality recordings. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition as a tagging problem\n",
    "*\n",
    "Speech recognition can be viewed as a generalization of the tagging problem:  Given an acoustic ouptut $A_{1,T}$ (consisting of a sequence of acoustic events $a_1,\\dots,a_T$) we want to find a sequence of words $_{1,R}$ that maximimizes the probability.*\n",
    "\n",
    "*Using the standard reformulation for tagging by rewriting  this by Bayes Rule and then dropping the denominator since it doesn't affect the maximisation, we transform the problem into computing.*\n",
    "\n",
    "$$argmax_wP(A_{1,T}|W_{1,R}){1,R})$$\n",
    "*\n",
    "In the speech recognition work, $P(W_{1,R})$ is called the **language model** as before and $P(A_{1,T}|W_{1,R})$ is called the **acoustic model**.*\n",
    "\n",
    "*This formulation so far, however, seems to raise more questions that answers.  In particular, we will address the main issues briefly here and then return to look at them in detail in the following sections.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a speech signal look like?\n",
    "*\n",
    "Human speech can best be explored by looking at the intensity at different frequencies over time.  This can be shown graphically in a spectogram of signal containing one word such as shown in Figure 1 below.\n",
    "*\n",
    "![Figure 1](https://selene.hud.ac.uk/u1273400/images/seg_media/spectogram.PNG)\n",
    "**Figure 1:** Spectogram of the word \"sad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Time is one the x-axis, frequency is on the y-axis, and the darkness of the area corresponds to intensity.  This word starts out with a log of high frequency noise with no noticiable lower frequencies of resonance (typical of an /s/ or /sh/ sound) startng at time 1.55, then at 1.7 there is a period of silence.  This iniital signal is indicative of a \"stop\" consonant such as a /t/, /d/, /p/.  Betwen 1.8 and 1.9 we see a typical vowel, with strong lower frequencies and distinctive bars of resonance called the **formants** clearly visible.  After 1.9 it looks like another stop consonant, that includes an area with lower freqeuncy and resonance.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sounds of Language\n",
    "*\n",
    "The sounds of language are classified into what are called **phonemes**.  A phoneme is a minimal unit of sound that has semantic content e.g. the phoneme AE versus the phoneme EH captures the difference between the words \"bat\" and \"bet\".  Not all acoustic changes change meaning.  For instance, singing words at different notes doesn't change meaning in english.  Thus changes in pitch does not lead to phenemic distinctions.\n",
    "*\n",
    "\n",
    "*Often we talk of specific features by which the phonemes can be distinguished.  One of the most important features distinguishing  phonemes is **voicing**.  A voiced phoneme is one that involves sound from the vocal cords.  For instance, F (e.g. \"fat\") and V (e.g. \"van\") differ primarily in the fact that in the latter, the vocal chords are producing sound, i.e. it is voiced, whereas, the former does not i.e. unvoiced.  Here's a quick breakdon of the major classes of phonemes.\n",
    "*\n",
    "\n",
    "#### Vowels\n",
    "*\n",
    "Vowels are always voiced, and the differences in English depend mostly on the formants (prominent resonances) that are produced by different positions of the tongue and lips.  Vowels generally stay steady  over the time they are produced by different positions of the tongue and lips.  Vowels generally stay steady over the time they are produced.  By holding the tongue to the front and varying its height we get vowels IY (beat), IH (bit), EH (bat), and AE (bet).  ith tongue in the mid position, we get AA (bob - bahb), ER (bird), AH (but), and AO (bought), With the tongue in the back position we get UW (boot), UH (book), and OW (boat).  There is another class of vowels called **diphthongs**, which do change during their duration.  They can be thought of as starting with one vowel and ending with another. For example AY (buy) can be approximated by starting with AY (bob) and ending with IY (beat). Similarly, we have AW (down, cf. AA W), EY (bait, cf. EH IY), and OY (boy, cf. AO, IY).\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consonants\n",
    "*\n",
    "Consonants fall into general classes, with many having voiced and unvoiced members*\n",
    "##### 1. Stops or Plosives\n",
    "*These all involve stopping the speech stream (using lips, tongue, etc.) and then rapidly releasing a stream of sound.  They come in voiced, unvoiced pairs. P and B, T and D, and K and G.*\n",
    "##### 2. Fricatives\n",
    "*These all involve \"hissing\" sounds generated by constraining the speech streem by the lips, teeth etc.  They also come in unvoiced and voiced pairs. F and V, TH and DH (e.g. thing versus that), S and Z, and finally SH and ZH (shut vs azure).*\n",
    "##### 3. Nasals\n",
    "*These are all voiced and involve moving air through the nasal cavities by blocking it with the lips, gums, and so on.  We have M, N and NX (sing).*\n",
    "##### 4. Affricatives\n",
    "*These are like stops followed by a fricative: CH (church), JH (judge)*\n",
    "##### 5. Semi-vowels\n",
    "*These are consonants that have vowel-like characteristics and include W, Y, L, and R.*\n",
    "##### 6. Whisper - H\n",
    "\n",
    "*This is the complete set of phonemes typically identified for English*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we map from continuous speech signal to a discrete sequence\n",
    "*\n",
    "One of the common approaches to mapping a signal to discrete events is to define a set of symbols that correspond to useful acoustic features of the signal over a short time interval.  We might first think that phonemes are the ideal unit, but they turn out to be far too complex and extended over time to be classified with simple signal processing techniques.  Rather, we need to consider much smaller intervals of speech (and then, as we see, model phonemes by HMMs over these values).  Typically, speech signal is divided into very small segments (e.g. 20 ms), and these intervals often overlap in time shown in Figure 2.  These segments are often calassified into different types, each corresponding to a symbol in a vocabulary called a **codebook**.  Speech systems typically use a vocabulary size between 256 and 1024.  Each symbol in the codebook associated with a vector of acoustic features that define its prototypical acoustic properties.  Rather than trying to define the properties of codebook symbols by hand, clustering algorithms are used on training data to find a set of properties that best distinguish the speech data.  To do this, however, we first need to design a representation of the speech signal as a relatively small vector of parameters.  These parameters must be chosen so that they capture the important distinguishing characteristics that we use to classify different speech sounds.  One of the most commonly features record the intensity of the signal over various bands of frequencies.  The lower bands help distinguish parts of voiced sounds, and the upper help distinguish fricatives.  In addition, other features capture the rate of change of intensity from one segment to the next.  These \"delta\" parameters are useful for identifying areas of rapid change in the signal, such as the points where we have stops and plosives.  Details on specific usueful representations and the training algorithms to build prototype vectors for codebook symbols will be presented.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we map from acoustic events (codebook symbols) to words?\n",
    "*\n",
    "Since there are many acoustic events than there are words, we need to introduce intermmediate structure to represent words, we need to introduce intermediate structures to represent words.  Since words will correspond to codebook symbol sequences, it makes sense to use an HMM for each word.  These word HMMs then would need to be combined witht he language model to produce an HMM model for sentences.  To see this technique, consider a highly simplified HMM model for a word such as \"sad\", and a bigram language model for a language that contains the words \"one\", \"sad\", and \"but\", as shown in figure 2. We can construct a sentence HMM by simply inserting the word HMM where we have substituted just the word \"sag\".  To complete the process we'd do the same from bus and one.  Note that the probabilities from the bigram model are used now to move between the word networks.\n",
    "*\n",
    "![Figure 2](https://selene.hud.ac.uk/u1273400/images/seg_media/lmdl.PNG)\n",
    "**Figure 2:** Simple language and word model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we train large vocabularies? Applications: Sub word models\n",
    "\n",
    "When we are working with applications of a few hundred words, it is often feasible to obtain enough training data to train each word model individually.  As applications grow into the thousands of words, or tens of thousands, it becomes impractical to obtain the necessary training data.  Say we need a corpus that has at least 5 instances of each word in order to train reasonably robust patterns.  Then for a ten thousand word vocabulary we would need well over 50,000 training words as the words will not be uniformly distributed.  Besides being impractical to collect such data, this method would lose all the generalities that could be exploited between words that contain similar sound patterns, and make it very difficult to design systems that could adapt to a particular speaker.  Thus, large-vocabulary speech recognition systems do not use word based models, but rather use subword models.\n",
    "\n",
    "The idea is that we build HMM models of words, which can then be reused in different word models.  There is a range of possibilities for what to bse the wubword units on, including the following.\n",
    "- phoneme based models\n",
    "- syllable based models\n",
    "- demi-syllable based models, where each sound represents a consonant cluster preceeding a vowel, or folloing a vowel;\n",
    "- phonemes in context, or triphones: context dependent models of phonemes dependent on what preceedes and follows the phoneme.\n",
    "\n",
    "There are advantages and disadvantages of each possible subword unit.  Phoneme-based models are attractive since there are typically few of them (e.g., we only need 40 to 50 for English).  Thus we need relatively little training data to construct good models.  Unfortunately, because of co-articulation and other context dependent effects, pure phoneme-based models do not perform well.  Some tests have shown they have only half the word accuracy of word bsed models.\n",
    "\n",
    "Syllable-based models provide a much more accurate model, but the prospect of having to train the estimated 10,000 syllables in English makes the training problem nearly as difficult as the word bsed models we had to start with.  In a syllable based representation, all single syllable words such as sad, would of course be represented identically to the word based models.\n",
    "\n",
    "Demi-syllables produce a reasonable compromise.  Here we'd have models such as /BAE/ and /AE D/ combining to represent the word bad.  With about 2000 possibilities, training would be difficult but not impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triphones (or phonemes in context, PIC) produce a similar style of representation to demi-syllables.  Here we model in terms of its context (i.e. phones on each side).  The word bad might break down into the triphone sequence $SIL^BAE\\text{ }B^{AE}D\\text{ }AE^DSIL$ i.e., a B preceded by a silence and followed by AE, then AE preceded by B and followed by D, and then D preceded by AE and followed by silence.  Of course, in continuous speech, we might not have a silence before or after the word, and we'd then have a different context depending on the words.  For instance, the triphone stiring Bad art might be  $SIL^BAE\\text{ }B^{AE}D\\text{ }AE^DAA\\text{ }D^{AA}R\\text{ }AA^RT\\text{ }R^TSIL$.  In other words, the last model in **bad** is a D preceded by AE and followed by AA.  Unfortunately, with 50 phonemes, we would have $50^3=125,000$! possible triphones.  But many of these, thankfully, don't occur in English.  Also we can reduce the number of possibiities by abstracting away the context parameters.  For itance, rather than using the 50 phonemes for each context, we use a reduced set including categories like silence, vowel, stop, nasal, liquid consonant, etc. (e.g. we hav an entry like $V^TV$, a /T/ between two vowels, rather than $AE^TAE, AE^TAH, AE^TEH,\\dots$ and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Whatever subword units are each, each word in the lexicon is represented by a sequence of units represented as a FSM or a Markov Model.  We then have three levels of Markov and Hidden Markov models which need to be combined into a large network. For instance Figure 3 shows the three levels that might be used for the word **sad** with a phoneme based model.\n",
    "![Figure 3](https://selene.hud.ac.uk/u1273400/images/seg_media/shmm.PNG)\n",
    "**Figure 3**: Three-level phoneme-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To construct the exploded HMM, we would first replace the phoneme nodes in the word model for **sad** with the appropriate phoneme mdoels as shown in Figure 4, and then use this network to replace the node for **sad** in the language model as shown in Figure 5.  Once this is down for all nodes representing words, we would have single HMM for recognising utterances using phoneme based pattern recognition.\n",
    "![Figure 4](https://selene.hud.ac.uk/u1273400/images/seg_media/sadm.PNG)\n",
    "**Figure 4**: The constructed word model for sad![Figure 4](https://selene.hud.ac.uk/u1273400/images/seg_media/sadx.PNG)\n",
    "**Figure 5**: Language model with expanded sad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find word boundaries?\n",
    "\n",
    "This in theory ha been solved. If we can run a Viterbi algorithm on a network such that in Figure 5, the best path will identify where the words are (and in fact provide a phonemic segmentation as well).\n",
    "\n",
    "### How can we train and search such a large network?\n",
    "\n",
    "While we have already examined methods for searching and training HMMs, the scale of the networks used in speech recognition are such that some additional techniques are needed.  For instance, the networks are so large that we need to use a beam or best-first search strategy, and often will build the networks incrementally as the search progresses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
