{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocoder\n",
    "*\n",
    "There are many different types of code that use analysis/synthesis.*  The major ones are discussed in the following sections. *With all these coders, data are coded into frames representing speech spectra measured at intervals of 10-30ms.  There are also techniques for efficient coding of frames and sequences of frames, and some specialized vocoders which code whole sequences of frames  as single units.  Most vocoders are based ona  modedl of speech production which exploits the fact that it is possible to substiantially separate to operations of sound generation and subsequent spectrum shaping.  The sources of the sound generated are modelled by periodic or random excitation and in several more recent vocoders it is also possible to have mixtures of both types of excitation.  The excitation is used as input to a dynamically controllable filter.   The filter systme models the combined effects of the spectral trend of the original sound source and the frequency response of the vocal tract.  The specifications for the sound sources and for the spectral envelope are both derived by analysis of the input speech.  By separating the fine structure specification of the sound sources from the overall spectral envelope description and identifying both in terms of a faily small number of slowly varying rates of 1,000-3000 bits/s.  *\n",
    "\n",
    "### Channel Vocoder\n",
    "*\n",
    "In a channel vocoder (of which Dudley's was the first example)  the spectrum is represented by the response of a bank of contiguous variable gain bandpass filters.   The way in which the desired overall response can be approximated using the seperate contributions from individual channels.  The control signals for the channels are derived by measuring the short-term-average power from a similar set of filters fed with the input speech signal in the transmitter.\n",
    "*\n",
    "\n",
    "*\n",
    "Unless a very large number of chanlles can be used (with consequent digit rate) it is difficult to achieve a good match to the spectrum shapes around the formant peaks with a channel vocoder.  However, the qualityh achievable with around 15-20 channels is resonable for communications purposes, and has been achieved in several systems operating at data rates of around 2,400 bits/s or lower.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinusoidal Vocoders\n",
    "*\n",
    "The key feature of sinusoidal analysis/sysnthesis models is the concept of representing the short-terms spectrum of a speech signal as a sum of sinousoids specified in terms of freequency, amplitude and phase.  One of such coding method is known as **sinusoidal transform coding (STC)**.  For each frame, a set of frequencies, amplitudes and phases is estimated corresponding to peaks in the short-term Fourier transform.  An economical code is achieved by representing voiced speech as a constrained set of harmonically related sinusoids, and unvoiced speech as a set of sinusoids with appropriately defined random phases.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiband Vocoders\n",
    "*\n",
    "**Multiband excitation (MBE) coding** is another method that uses sinusoidal model.  The main features of the MBE approach that dinstinguishes it from STC are the treatment of the excitation and the method used to generate unvoiced spectrum is divided into harmonic bands and a binary voiced/unvoiced decision is made separately for each band.  The voiced components of the speech are regenerated as a combination of the relevant set of harmonic sinusoids, with the amplitudes of all sinusoids associated with an 'unvoiced' harmonic being set to zero. The unvoiced components are generated separately using a frequency-domain method.  In this method, the spectrum shape taken from the unvoiced samples of the estimated vocal tract spectral envelope is multiplied by the spectrum of a white noise excitation signal.  The resulting transform is used to sysnthesize the unvoiced speech signal components, which are added to the voiced components to produce the final synthesized speech waveform.\n",
    "*\n",
    "\n",
    "*\n",
    "Efficient coding techniques have been developed both for STC and for MBE coding, and have enabled these methods to achieve considerable success at data rates in the 2,000-4,000 bits/s range, providng more natural-sounding speech than traditional channel vocoders at these data rates.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPC Vocoders\n",
    "\n",
    "*\n",
    "In LPC vocoders, spectral approximation to a speech signal is given by the response of a sampled-data filter, whose all-pole transfer function is chosen to give a least-squared error in a waveform prediction.  \n",
    "*\n",
    "\n",
    "*The principle of linear prediction applied to a resonant system depends on the fact that resonance causes the future output of a system to depend on its previous history, because resontant modes continue to 'ring' after their excitation has ceased.  The characteristics of this ringing are determined by a linear difference equation of appropriate order.  If a system is ringing entirely as a result of previous excitation, and can be represented exactly by  a small number of resonant modes with constant characteristics, a difference equation can be derived to predict its future output exactly.  In speech, however, these assumptons are only approximately true.  Althrough the formants cause a strong resonance effect, they vary slowly with time.  Their damping is also modified by opening and closing of the glottis.  The resonances are alwyas being excited to some extent by the sound sources, and receive substantial excitation at the instants of glottal clusure.  In spite of all these deviations from the ideal, it is possible to derive a useful description of the spectrum by choosing parameters of a predictor filter to minimize the average prediction error power over a frame of input samples around 10-20ms duration.   The predictor filter is a finite-impulse response filter whose output is a weighted linear combination of previous input speech samples.  At the receiver, the predictor is connected in a feedback loop to give an all-pole recursive filter, whose resonant modes approximate those for the input signal.   The two main methods that are used for deriving the predictor filter coefficients are known as the covariance and autocorrelation methods.\n",
    "*\n",
    "\n",
    "*In any practical LPC-based coding system, the parameters describing the predictor filter must be quantised prior to transmission to the decoder.  The filter coefficients are not quantized directly, because small quantisation errors can give rise to large changes in the spectral response of the filter.  The coefficients are therefore converted to some some alternative representation, which is chosen to be more robust under quantisation.  For recent implementations, including the LPC derived intermediate-rate coders, the most popular choice is a paremter set called line spectral pairs.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When the predictor filter has been adjusted to predict the input as best it can from the immediately preceding samples, the difference between  the input speech and the predictor output (known as the residual) will have a roughly flat spectrum.  The obvious spectral peaks caused by the resonances of speech production will have been removed.  For this reason the complete filtering process is sometimes referred to as inverse filtering.*\n",
    "\n",
    "*In LPC vocoders, the resonant properties of the synthesis filter make possible fairly good approximations to the spectral shapes of the formants.  However, the correct analysis to achieve this result will only be obtained when the overall speech spectrum really is like the repsonse of an all-pole filter.  During vowel sounds this approximation is often very close, although the normal frame rates standard LPC cannot deal correctly with the fact that hte formant bandwidths in natural speech are frequent other occassions when the spectral modelling is quite poor, particularly during nasalized vowels and many voiced consonant sounds.  On these occasions the LPC synthesis frequency produces spectral peaks whose bandwidths are too large, with a consequent 'buzziness' in the speech quality.  Another inherent property of normal LPC vocoders is that all regions of the spectrum are treated equally with regard to accuracy of frequency specifications, and so no advantages is taken of the variable frequency resolution of auditory perception.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Simple LPC coding produces speech which is buzzy but intelligible, and a version of the algorithm was used for many years as a U.S. Government standard at 2,400 bits/s for secure voice communications in both military and civilian use.  Recently, the standard has been replaced by a new standard at the same data rate but using an enhanced LPC algorithm that gives speech of considerably better quality.  This algorithm is known as mixed excitation linear prediction MELP, uses a frequency-dependent mixture of pule and noise excitation for a parallel-formant synthesizer.  Other features of MELP coder include a spectral enhancement filter to improve dispersion filter applied after the LPC synthesis filter in order to improve the modelling of regions betwen the formants.  The new developments have removed a lot of the buzziness traditionally associated with LPC-coded speech.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formant Vocoders\n",
    "*\n",
    "In formant vocoders, the spectrum shap is specified in terms of frequencies and apmplitudes associated with the resonant modes of the speaker's vocal tract.\n",
    "*\n",
    "\n",
    "*Formant vocoders are different from other types described above, as they use a synthesizer that is much more closely related to human speech production.   In addition to modelling periodic and noise sources, the synthesizer has a spectral filter system with resonators that are explicitly related to the principal formants of the input speech.  Thus the coding system can be constrained to deal only with the knon frequency range and necessary accuracy of specification for each formant.   The systematic variatio of formant bandwidth with glottal opening can easily be provided in a formant synthesizer and requires no extra information to be transmitted.  Apart from this effect, the bandwidths of the formants do not vary much and as such variation as does occur is fairly predictable; provided they are within the limits of natural variation, preservation of the actual formant bandwidths is not perceptually important.  In consequence this property of the resonances is not usually transmitted in formant vocoders.  For formant vocoders it is not practicable to use a simple cascade connection of resonantors, because occasional errors of formant frequency could then cause serious formant amplitude errors.*\n",
    "\n",
    "*Analysis is the main difficulty with formant vocoders.  When the spectral envelope of a speech sound shows a small number of well-defined peaks it is trivial to assign these to formants in a sensible way.  However, there are occasions, particularly in consonants, near vowel/consonant boundaries or even in the middle of vowels if the fundamental frequency if very high, when it is not clear what is the most appropriate way to asign the parameters of the synthesizer to spectrial peaks of the input signal.  Because of the analysis difficulties formant vocoders are notyet used operationally, but few have been demonstrated in a research environment and some have been extremely successful though computationally expensive.  Formant analysers have been used to derive stored components for message synthesis, because the analysis can then be carried out more slowly than real time on only a moderate amount of speech material, and serious analysis errors can be corrected by subsequent interactive editing.  This possibility is of course, not available for coding real-time conversation.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Parameter coding\n",
    "*\n",
    "For all types of analysis/synthesis systems it is possible to achieve some saving in digit rate by exploiting redundancy in the measured parameters.  Any technique of this type will add complexxity, but the analysis process itself gives such a reduction of digit rate compared with the original speech that the computational speed needed for further processing can be quite low.  with modern implementation technology farly complex coding is possible even in single-chip microprocessors.*\n",
    "\n",
    "*Vector quantisation (VQ) considers only one frame at a time and exploits the fact that the multi-dimensional parameter space is not uniformly occupied.  By choosing from a subset of possible combination of parameter values (which are stored in a codebook), fewer bits are required per frame than are needed from for independent coding of the parameters.  However, quite a lot of computation is needed to select the most appropriate member of the codebook for each frame.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocoders based on segmental/phonetic structure\n",
    "\n",
    "*Even lower bit rates can be achieved by coding whole sequences of frames as single units.  In segment vocodoers, consistent segments of speech are identified and an extended form of VQ is used to provide a compact codebook that contains variable-length segments.  An utterance can then be coded by finding the sequence of these segments that provides the best fit according to some suitable distance criterion.  Segment vocoders have been produced with rasonable intelligibility at rates of less than 300bits/s.  Howver, they work best when used to code speech from a known speaker, so that a speaker-specific codebook can be used.  If coding of speech from any (unknown) speaker is required, it becomes difficult to retain speaker characteristics while keeping the codebook at a manageable size.*\n",
    "\n",
    "*The lowest bit rates are possible by explicitly taking advantage of the phonetic structure of spoken language and using a phoneme-based coding scheme.  For example, the English language contains about 44 phonemes (which can be generously specified using 6-bits) and a typical speaking rate might be an average of around 12 phonemes/s, the phoneme sequence can be coded at a rate of approximately 70bits/s.  Including some additional bits for pitch and timing informatio may only increase the bit rate to not much more than 100 bits/s.  The term phonetic vocoder is generally used to describe a type of segment vocoder in which sigements are explicitly phonetic terms.  These coders involve applying automatic speech recognition at the transmitter and using some synthesis technique at the receiever, and there therefore referred to as recogntion synthesis coders.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
