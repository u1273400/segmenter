{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW WORKFLOW FOR COMPUTER SCIENCE RESEARCH\n",
    "\n",
    "## JOHN ALAMINA\n",
    "### NOVEMBER, 2015\n",
    "\n",
    "*\"Separation of concerns makes for good modular design - Can there be exceptions to this rule?\"*\n",
    "\n",
    "## Contents<a name=\"contents\"></a>\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "2. [Disclaimer](#disclaimer)\n",
    "3. [Introduction](#intro)\n",
    "3. [The new workflow](#workflow)\n",
    "4. [GIT & GitHub](#git)\n",
    "5. [Jupyter](#jupyter)\n",
    "6. [Markdown](#markdown)\n",
    "7. [Workflow Tasks](#tasks)\n",
    "7. [A case for Neural Networks](#case)\n",
    "8. [Keeping it simple - *hopefully*](#simple)\n",
    "9. [Concluding Remarks](#remarks)\n",
    "10. [Appendix I - Git commands](#A1)\n",
    "11. [Appendix II - Useful links](#A2)\n",
    "\n",
    "\n",
    "## [Prerequisites](#contents)<a name=\"prerequisites\"></a>\n",
    "1. GIT\n",
    "2. GITHUB account\n",
    "\n",
    "## [Disclaimer](#contents)<a name=\"disclaimer\"></a>\n",
    "#### Warning: This is a quick tutorial and this material is experimental, not exhaustive and may appear to have content that may seem to be more of ‘academic hipsterism’ and as such may not appear to find immediate advantage nor usability.  However, once successfully applied, may also be addictive with a ‘no turning back’ tendency. A middle ground is therefore sought at the end but this is only an attempt to find and evolutionary reason for adoption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction(#contents)<a name=\"intro\"></a>\n",
    "By way of introduction let's critically think about the computer science school of thought of separation versus integration referred to in the presentation title. From a programmer perspective there is an advantage of simplicity and clarity.  However, that is about it. Enter concurrency. For much of the recent years advances in computer hardware (and consequently software) has been due to this concept of concurrency and things being done in parallel.  The reasons for this is obvious; the gains in speed.  This however comes at the cost of losing simplicity.\n",
    "\n",
    "Have you ever written a program using MsWord before? Those two never mix however at the end of the day we would need to document our programs using one method of word processing or another.  Traditionally speaking, we never mix code with formatted documents.  Not all scholars would agree with thise schema,  especially those who practice well documented code.  It is also no surprise that most developers are poor code documenters.  In production environments however, it can be seen that well documented well-documented code pays off in the long run than poorly documented code (ref).  If this is the case, then would it be sufficient to say that running elaborate code documentation in parallel with the development process is an effective strategy for efficient code production and well documented code? Further, what if there was a means to mix code and documentation in order to achieve this.  \n",
    "\n",
    "This presentation proposes a new workflow for achieving better documentation for computer science research that more often than not involves software programming of code.  It considers Git-Jupyter combination cloud/web-based platform for achieving this integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [The New Workflow](#contents)<a name=\"workflow\"></a>\n",
    "Well, if you are viewing this document then you are already viewing the new workflow.  It consists of the following technlogies:\n",
    "1. Git\n",
    "2. GitHub\n",
    "3. Jupyter\n",
    "4. Markdown\n",
    "5. MathJax \n",
    "\n",
    "### Workflow Features\n",
    "1. Mixture of Code and documentation\n",
    "2. Ability to debug and run code\n",
    "3. Ability to format documents\n",
    "4. Cloud-based/Desktop (web) based\n",
    "5. Ability to include links and images\n",
    "6. Ability to add equations using Latex, ascimath and MathML\n",
    "7. Code collaboration and version control using Git & GitHub\n",
    "\n",
    "### Advantages\n",
    "- Early integration of code and documentation\n",
    "- Elaborate Documentation\n",
    "- Test Driven Development\n",
    "- Better code interaction\n",
    "- On demand documentation\n",
    "- Cloud platform perks - security, scalability, backup\n",
    "- Better collaboration\n",
    "- Improved productivity via integrated workflow\n",
    "\n",
    "Disadvantages will be considered at the concluding remarks.\n",
    "\n",
    "### [Workflow Tasks](#contents)\n",
    "1. **Veiwing the repository on GitHub**\n",
    "2. Cloning the Repository locally\n",
    "3. Editing Files with Jupyter\n",
    "4. Adding, Committing and Pushing files back to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GIT/GitHub](#contents)<a name=\"git\"></a>\n",
    "Git is a version control system that has been made popular by GitHub, which is an online repository for open-source software.\n",
    "\n",
    "### Features\n",
    "1. Code Backups\n",
    "2. Code Version Comparisons\n",
    "3. Cloud sync and Collaboration\n",
    "\n",
    "### Advantages\n",
    "1. Compatible with Markdown & ipython notebooks i.e. textual representations\n",
    "2. Automatic document and code versioning\n",
    "3. Cloud backup\n",
    "4. Automatic (automagic) rendering of Jupyter notebooks\n",
    "\n",
    "### [Workflow Tasks](#contents)\n",
    "1. Veiwing the repository on GitHub\n",
    "2. **Cloning the Repository locally**\n",
    "3. **Editing Files with Jupyter**\n",
    "4. Adding, Committing and Pushing files back to GitHub\n",
    "\n",
    "## [Jupyter](#contents)<a name=\"jupyter\"></a>\n",
    "Jupyter is an acronym that was formed from the fusion of three scripting languages being fused into one environment namely\n",
    "- Julia\n",
    "- Python and\n",
    "- R\n",
    "### Features\n",
    "1. Web based environment for scripting languages\n",
    "2. Supports elaborate and extensive in-file documentation\n",
    "3. Supports web-based formatting using mark down\n",
    "4. Visualization through various libraries\n",
    "5. Evolutionary Multiple scripting languages (up to 40)\n",
    "6. Access to operating system command line\n",
    "7. Cell based processing\n",
    "8. Support Latex style equation using MathJax\n",
    "\n",
    "### Advantages\n",
    "1. Integrated Development of Code and documentation\n",
    "2. Rich visualisation\n",
    "3. Cloud based computing support\n",
    "4. Platform Independence\n",
    "5. Language neutrality (to a good extent)\n",
    "6. Support of equations using MathJax\n",
    "\n",
    "## [MarkDown](#contents)<a name=\"markdown\"></a>\n",
    "Markdown is a counter intuitive means of authoring documents on the web. So rather than using markup langauges which tends to make the document spagetty, we use a much simpler means of formatting that is less burdensome to the documenter.\n",
    "\n",
    "### Features\n",
    "1. Supports common useful formatting\n",
    "2. Headings, emphasis, strong\n",
    "3. List and numbering\n",
    "4. Tables\n",
    "5. Links\n",
    "6. Image/Video embedding\n",
    "7. Escape for format characters\n",
    "\n",
    "### Advantages\n",
    "1. Simplicity\n",
    "2. Others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Workflow Tasks](#contents)<a name=\"tasks\"></a>\n",
    "1. Veiwing the repository on GitHub\n",
    "2. Cloning the Repository locally\n",
    "3. Editing Files with Jupyter\n",
    "4. **Adding, Committing and Pushing files back to GitHub**\n",
    "5. ** Pulling from Github **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [A case for Neural Networks](#contents)<a name=\"case\"></a>\n",
    "\n",
    "Contrastive Divergence is an an efficient learning algorithm that allows Deep Belief Networks learn multi-layer generative models from unlabeled data.  This greedy layer-wise pretraining method can be used to initialize a neural network that is fine-tuned with back-propagation (Sarikaya, Hinton & Deoras, 2014).\n",
    "\n",
    "\n",
    "\n",
    "Neural Networks initialised by Deep Belief Networks were found to perform as well as those that use other algorithms such as Supprt Vector Machines (SVMs), Boosting and Maximum Entropy.  However, these other algorithms depend on supervised learning of pre-classified data.  DBNs on the other hand have the benefit of also accurately classifying unlabeled data.\n",
    "\n",
    "Up until recently, Neural networks efficiency was impeded by difficulty of training when compared to their probabilistic couterparts such as Hidden Markov Models. In order to overcome this limitation an optimized probabilistic algorithm known as the Restricted Boltzmann Machine (RBM) was invented to incorporate efficient probabilistic training methods using Deep Belief networks to provide fast training algorithms. In other words,Saraki et. al. (2014) assert that DBNs are analogous to Maximum entropy classifiers whose features are being learned rather than being preset.\n",
    "\n",
    "### NAIVE BAYES\n",
    "The Naive Bayes is a supervised, probabilistic classifier.  It does well with data whose inputs are independent from each other and biased towards problems where each variable probability is above zero.  The Naive Bayes probability uses conditional probability to make predictions about future outcomes.  Conditional probabilities involve at least two variables and measures the probability of one or more events based on the presence or absence of one or more other separate variables.  The Naive bayes derivation is better understood from the perspective of the chain rule.\n",
    "\n",
    "\n",
    "\n",
    "#### CHAIN RULE\n",
    "This is the probability that three or more events will occur simultaneously. The general case of the chain rule is expressed as follows:\n",
    "\n",
    "$ P(A_1,A_2, ... , A_n)= P(A_1)P(A_2|A_1)P(A_3|A_2,A_1)...P(A_n|A_{n-1}, ... , A_1) $\n",
    "\n",
    "\n",
    "The Naive Bayes takes advantage of the interplay between conditional probability and joint probability.  Thus, on the left hand side of the equation we hold one of these variables as independent while the others are dependent on the independent variables whilst independent of each other.  So the equation now becomes.\n",
    "\n",
    "$ P(A_1|A_2, ... , A_n)= P(A_1)P(A_2|A_1)P(A_3|A_1)...P(A_n|A_1) $\n",
    "\n",
    "\n",
    "### MAXIMUM ENTROPY CLASSIFIERS\n",
    "\n",
    "*Maximum Entropy based classifiers do not assume statistical independence of the features that are used as predictors.  As such, they allow the combination of multiple overlapping information sources.  The information sources are combined as follows:*\n",
    "\n",
    "$  \\frac{e^{ \\sum_i\\lambda_{i}f_i(C,W)} }{\\sum_{C'} e^{ \\sum_i\\lambda_{i} f_i(C,W)}} $\n",
    "\n",
    "*which describes the probability of a particular class C (e.g. call-types) given the word sequence  spoken by the caller.  Notice that the denominator includes a sum over all clases C', which is essentially a normalisation factor of for probabilities to sum to 1.  The f<sub>i</sub> are indicator functions, or features, which are â€œactivatedâ€ based on computable features on theword sequence,\n",
    "for example if a particular word or word pair appears, or if the parse tree contains a particular tag, etc. The MaxEnt models are trained using the improved iterative scaling algorithm [21] with Gaussian prior smoothing [20] using a single universal variance parameter of 2.0.*\n",
    "\n",
    "### Boosting\n",
    "*Boosting is a method that can be used in conjunction with many learning algorithms to improve the accuracy of the learning algorithm. The idea of Boosting is to produce an accurate prediction rule by combining many moderately inaccurate (weak) rules into a single classifier. At each iteration,\n",
    "boosing adds a new (weak) prediction rule that focuses on samples that are incorrectly classified by the current combined predictor. Even though Boosting is known to be sensitive to noisy data and outliers, in some problems, it is less susceptible to overfitting than most machine learning algorithms.*\n",
    "\n",
    "*We used a specific implementation of Boosting, AdaBoost using decision stumps, which is described in [6]. Boosting has been applied to a number of natural language processing tasks in the past [9].*\n",
    "\n",
    "### Support Vector Machines\n",
    "*Support vector machines (SVMs) are supervised learning methods used for classification. The basic SVM takes a set of input data and predicts, for each given input, which of two possible classes forms the output, making it a non-probabilistic binary classifier.  SVMs are derived from the theory of structural risk minimization[7]. SVMs learn the boundaries between samples of the two classes by mapping these sample points into a higher dimensional space. SVMs construct a hyperplane or a set of hyperplanes in a high-dimensional space, which can be used for classification. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training\n",
    "data point of any class (the â€œfunctional marginâ€), since in general the larger the margin the lower the generalization error of the classifier. The hyperplane separating these regions is found\n",
    "by maximizing the margin between closest sample points belonging to competing classes. In addition to performing linear classification, SVMs can efficiently perform non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Much of\n",
    "the flexibility and classification power of SVMs resides in the choice of kernel. Some of the commonly used kernels are linear, polynomial and radial basis functions. In this work, we chose linear kernels to train the SVM since computationally it is faster compared to other kernels, yet there is no significant difference in performance for the current task. This is a fairly standard result for applying SVMs in natural language processing since we are already using a high-dimensional feature vector.*\n",
    "\n",
    "### REFERENCES\n",
    "\n",
    "Sarikaya, R., Hinton, G., & Deoras, A. (2014). Application of Deep Belief Networks for Natural Language Understanding. IEEE/ACM Transactions On Audio, Speech, And Language Processing, 22(4), 778-784. http://dx.doi.org/10.1109/taslp.2014.2303296\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Keeping it Simple - *hopefully*](#contents)<a name=\"simple\"></a>\n",
    "- Coming back to Word Processors - such as Ms Word\n",
    "\n",
    "## [Concluding Remarks](#contents)<a name=\"remarks\"></a>\n",
    "I promised to conclude this presentation by criticising the workflow at the same time however, I will attempt to use the criticisms to form a case for the new workflow by proferring solutions for the various points observed.\n",
    "\n",
    "### Criticisms of the Workflow\n",
    "#### 1.  The Workflow is Tedious\n",
    "True. The workflow requires the User to Learn myriad technologies - Git, jupyter, markdown, Latex in addition to multiple installations and configurations.  On the bright side, most of the underlying ideas behind the technologies involved are not at all new to us and further examination reveals that Git would have the steepest learning curve.  Overall, this tutorial was developed as a beginner to these technologies which is further proof that they aren't really hard afterall.\n",
    "\n",
    "#### 2. The Workflow is New and Subject to Sudden Changes\n",
    "The very same thing occurs in traditional software. This does however have the downside of being developed from various disparate sources coming together and therefore greater issues may arise.  For instance there are various versions of markdown that are not particularly compatible with each other.  However, as an evolving technological worlds we are learning to minimise the lack of standardisation as we have learnt through our experience of past technologies such as HTML and browser discrepancies.\n",
    "\n",
    "#### 3. Production Environment limitations\n",
    "I think this is the most crucial disadvantage.  The fact that this integrates documentation and coding means that the production environment limitation and two-fold.  With regards to documentation the formatting is biased towards web authoring unlike traditional word processors that are built for the printing hard copy documents.  As such desired features such as hard page breaks, citations and references.  We can fall back to word using our writage tool for this.  In addition, an online tool for achieving this once you are production ready (which I haven't included in the links section). \n",
    "\n",
    "### Future Prospects\n",
    "1. Markdown having more prominence in things such as text/instant messages\n",
    "2. Multiple Mark-down layouts such as presentation and spread sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Appendix I - Git Commands](#contents)<a name=\"A1\"></a>\n",
    "- git clone *url* (to make a local copy of a remote repository in an empty folder)\n",
    "- git checkout *branch* (checks out a branch/commit for editing)\n",
    "- git add *path* (adds a file/folder to the stage)\n",
    "- git commit -m '*message*' (commits stages files of a checked out branch)\n",
    "- git remote add *id url* (creates a remote id) \n",
    "- git push *remoteid branch* (updates remote repository)\n",
    "- git pull *remoteid branch* (updates local directory)\n",
    "\n",
    "## [Appendix  II - Useful Links](#contents)<a name=\"A2\"></a>\n",
    "- [Pandoc Installation](https://github.com/jgm/pandoc/releases/tag/1.15.2)\n",
    "- [MikTex Installation](http://miktex.org/download)\n",
    "- [Learn Git By Udacity](https://www.udacity.com/course/viewer#!/c-ud775/l-3105028581/e-3073678898/m-3073678899)\n",
    "- [Git Installer](https://confluence.atlassian.com/bitbucket/set-up-git-744723531.html)\n",
    "- [Learn Git Branching](http://cdn-thumbshot-ie.pearltrees.com/dd/f6/ddf6671bc8562cf5537625d27510bef8-b52square.jpg)\n",
    "- [Writage Installer](http://www.writage.com/)\n",
    "- [Git Command Cheatsheet](https://training.github.com/kit/downloads/github-git-cheat-sheet.pdf)\n"
    "- [Authorea](https://www.authorea.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# THANK YOU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
